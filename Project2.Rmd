---
title: "Project # 2: The Experiment Experiment"
author: "Sunny Kharel, William Easterby, Gabrielle Toutin"
date: "10/16/2020"
output: pdf_document
urlcolor: blue 
---

---

## Part One: Guided listening.


### Problem 1.1: What was it that prompted Brian Nosek to undertake the project described in the podcast?


When Brian Nosek read the paper published by the famous psychologist, he noticed that the
current constructs for something to be scientifically significant allowed for the absurd conclusion
that ESP is true. Knowing the absurdity of this claim, Nosek realized that the construct by which
the paper was able to validate this claim had an inherent flaw, and needed to be fixed.


### Problem 1.2: What did the project designed by Brian Nosek consist of?


The project designed by Nosek consisted of volunteer researchers who, combined, repeated
100 studies to see if they could reproduce positive results. Whether the result of the repeated
study matched the result of the previous one was kept in a spreadsheet.


### Problem 1.3: What is the (at least one) reason that scientists do not habitually repeat studies?


One reason scientists do not habitually repeat studies is because they rely on publishing new
findings to get tenure and other research positions. Confirming the results of old studies does
not lend itself to succeeding in one’s research career.


### Problem 1.4: How many experiments did the volunteer scientists “do over”?


The volunteer scientists redid 100 experiments.


### Problem 1.5: What was the source of the chosen experiments? Were they obscure within the field?


The source of the chosen experiments were three of the top psychology journals, so they were
not obscure within the field.


### Problem 1.6: What is the “afternoon-treat hypothesis”?


The after-noon treat hypothesis was the hypothesis that a sugar boost, e.g. a drink of sugary
lemonade, makes people engage in more effortful decision making.


### Problem 1.7: Did the project originator Brian Nosek keep constant track of how many of the experiments were successfully replicated? Or did he wait until the entire experiment experiment was completed?


The project originator Nosek waited until the entire experiment was completed to check whether
the experiments were successfully replicated.


### Problem 1.8: How many original conclusions were confirmed?


Only 39 out of the original 100 conclusions were confirmed.


### Problem 1.9: Is the conclusion that the scientists are faking their data?


Nosek does not believe that scientists are faking their data. Rather, he believes that a lot of
what is happening is statistical flukes.


### Problem 1.10: What experiment did the journalist conduct the morning of taping the podcast? What were the results?


The journalist flipped a coin 10 times in the morning and kept track of how many times he got
heads. The journalist got heads 9 times.


### Problem 1.11: What is the file-drawer effect? What is its consequence in the field of psychology?


The file-drawer is the effect that negative results for an experiment are likely to be put in a
file-drawer because journals are more likely to publish a positive result. Thus, although there are
usually more negative results, only the small percentage of positive results get published. In the
field of psychology, 97% of publications are positive results.


### Problem 1.12: Does the file-drawer effect completely explain the 39/100 ratio?


Nosek believes that the file-drawer did not completely describe the 39/100 ratio because he
thought that there must have been some error in the way that data was sampled in the
experiment design.


### Problem 1.13: Which example of a common mistake does Dr. Lindsey describe?


The common mistake that Dr. Lindsey describes is researchers trick themselves into adding
more samples to their experiment to increase their chances of getting a positive result, and stop
when the data proves them right. In reality this process only increases the chance that the
researcher will find a fluke as opposed to a scientific breakthrough. The reason behind this is
because there is an inner conflict of interest that publishing interesting results could lead to
advancement in one's career.


### Problem 1.14: Which other disciplines are now trying to do the experiment experiment?


The fields of economy, ecology, cancer biology, etc. are trying to do the experiment experiment.


### Problem 1.15: What remedy does Brian Nosek propose?


Nosek proposes a system where researchers are obligated to predetermine how one is going to
conduct an experiment, how they’re going to analyze data, and what they are going to learn
before a project. In this system, researchers are obligated to publish their results to a central
agency no matter what. In this way personal interest does not kick in when sampling data, and
the file-drawer effect is avoided because negative samples are published as well.


### Problem 1.16: Is this idea already being implemented in a certain research field? Has this changed the frequency of positive results?


Drug and medicine research made this mandatory. Before the registry was created, more than
half of the published studies of heart disease had positive results. After the registry was created
that number dropped down to 8%.


### Problem 1.17: Should we lose faith in scientific results?


We should not lose faith in scientific results because a systemized approach to publishing
scientific discovery can suppress the impact of flukes in the research world.


## Part 2: A simple experiment
!["A seven!"]("Seven day.jpg"){Width=10%}  
Every day since the middle of August this year, David Lynch (legendary director of Twin Peaks, Eraserhead, and many more) has been drawing balls labeled 1-10 from a jar that he built. Every video, he shows the numbered balls in the jar, swirls the numbers with his hand while not looking at them, and chooses a number. This number is deemed "Today's Number".  
After a while, people noticed that all the numbers had been chosen except for 7. Commenters on YouTube have questioned how fair his drawings are. Some hypothesized that he was placing the 7 in the fridge to deliberately avoid it.   
!["Man, if tomorrow's not a 7 I'm gonna lose it"]("Wes.jpg"){width=50%}  
One day, he drew a 7. There hasn't been a 7 since. Is he being fair in choosing his numbers?

###Analysis
```{r}
numbers = read.csv("David Lynch Numbers.csv")
hist(numbers$Outcome, breaks=0:10, xlab="Ball number", main="Histogram of ball numbers")
```

The data is expected to follow a uniform distribution because each ball has a 1/10 chance of being chosen. Given that there are `r length(numbers$Outcome)` days of outcomes, each number is expected to show up `r length(numbers$Outcome)/10` times.

###Distribution of frequencies
```{r}
frequencies = c()
for (i in 1:10)
{
  frequencies = c(frequencies,sum(numbers$Outcome==i))
}
hist(frequencies, breaks=0:12, xlab="# of times certain number appeared",ylab="How many numbers",main="Histogram of # of times a certain number appears")
```

Because the numbers are randomly chosen, you would expect the numbers to have frequencies normally distributed about 6.1. This is about what happens. This graph is very choppy, but there are 2 modes: 7 and 5. These are very close to 6.1.
```{r}
summary(frequencies)
```

Additionally, the mean frequency happens to be exactly 6.1! The median is 6, which is very close to this. The standard deviation is `r sd(frequencies)` but the IQR is `r 7.75-4.25`, indicating that the data is heavy towards the ends of the graph, as about half of the data could fit in 1 standard deviation.  

###Conclusion
I believe that David Lynch is being honest and drawing balls fairly. Both the mean and the median are close to the expected value. Although the data is spread out, I believe that this can be accepted due to the fact that there have only been 61 draws so far. As time goes on, I would expect that the frequency data would become more normally distributed, and that there will be more sevens!


## Part 3: Further reading

## Problem 3.1: Find **one** available study which was discussed in the podcast. Comment on the original and the replicated study.


One study mentioned in the podcast was one about the effects of sugary drinks on a person’s decision making. For both used the same experimental procedure, which include double-blind experimenting, and both used the same analysis procedures. Both experiments seemed to be conducted very well, with it seeming like they took care to not introduce any biases into the experiment. However, the replicated study did not reproduce the results of the original study. This goes a long with what the podcast talks about, making it appear as if the original study results may have just been statistical flukes. This idea aligns somewhat with what we have been learning about in class with confidence intervals, where even with a high interval there will be experiments where the true mean is not within the interval.


## Problem 3.2: Look into the following [article](http://www.statnews.com/2015/12/18/clinical-trial-reporting/). What is the take-home message?


The results of many clinical trials have gone unreported, violating a federal law which mandates it. This is bad, as it can cause the effects of drugs to be exaggerated and can hide some of the negative effects of them. Even more than just reporting the results, it is also important to publish them in a reader-friendly way and also to provide information on how the analysis was conducted, and perhaps to even publish the data for others to be able to analyze. This is something that is important not only to clinical trials, but to science as a whole.


## Problem 3.3: Look into the following [article](http://www.statnews.com/pharmalot/2015/12/16/pharmalot-nih-drug-trials/). What is the take-home message?


The number of industry-sponsored drug trails have increased in recent years, which is not great for the public because it can make it hard to decide which treatment is best. The issue is not that these industry-sponsored trials are bad, but instead that they may not tell the full story of the drug’s effects as they tend to only report the drug’s affects over a placebo and not against other available drugs. This can make it hard for physicians to decide which it best.


## Problem 3.4: Find out more about Andrew Wakefield. Then go to this [CDC website](https://www.cdc.gov/vaccinesafety/concerns/autism.html). What are your comments? 


Andrew Wakefield is known for publishing a study which showed a link between the MMR vaccine and autism which no one was ever able to reproduce the results of the study. It appears that he not only failed to disclose financial interests but also may have selectively chosen results that helped his conclusion. As the CDC reports, as well as many others, there is no actual link between vaccines and autism. This can help to show that even published results of a single experiment are not always reliable as if the experiment is not conducted properly and transparently, and if it cannot be reproduced, it can lead people to draw incorrect conclusions.

